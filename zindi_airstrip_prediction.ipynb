{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangchow/zindi-amazon-secret-runway/blob/main/zindi_airstrip_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CDbvN6Zoefb"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JN_NZz2XtgKm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install geopandas\n",
        "!pip -q install geojson\n",
        "!pip -q install rasterio\n",
        "!pip -q install tqdm\n",
        "!pip -q install segmentation-models-pytorch\n",
        "!pip -q install albumentations\n",
        "!pip -q install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ew3_MrrGslr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b674ae4-87ab-4ef5-8b93-fcbee7c8dd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Geospatial processing packages\n",
        "import geopandas as gpd\n",
        "import geojson\n",
        "\n",
        "import shapely\n",
        "import rasterio as rio\n",
        "from rasterio.plot import show\n",
        "import rasterio.mask\n",
        "from shapely.geometry import box\n",
        "from rasterio.io import MemoryFile\n",
        "from rasterio.transform import from_bounds\n",
        "\n",
        "# Mapping and plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as cl\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Utility libraries\n",
        "import glob\n",
        "import re\n",
        "import shutil\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create folder to store data products in Colab Runtime\n",
        "Create a `working` folder in Google Colab runtime for storing intermediate data products. This folder is deleted at the end of your session."
      ],
      "metadata": {
        "id": "oddD8RT6NvxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the new folder\n",
        "working_path = '/content/working'\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(working_path):\n",
        "    os.makedirs(working_path)\n",
        "    print(f\"Folder created: {working_path}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {working_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya0c4DY1NlPl",
        "outputId": "ec73e4bf-11f5-41cb-c385-3458670eed97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created: /content/working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data to local compute node"
      ],
      "metadata": {
        "id": "Xwh89QLIAFE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q7zyv6ZUASzC",
        "outputId": "c08f1195-b4d6-4125-c140-cf55b067cd2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download inference images from your Google Drive\n",
        "\n",
        "**Note:** If you are working with your own data, edit the path to your inference images in the cell below"
      ],
      "metadata": {
        "id": "M9GaZx0YAakO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the directory with inference images\n",
        "%cd /content/drive/MyDrive/Zindi-Amazon/inference\n",
        "# Zip the data\n",
        "!zip -r /content/inference.zip .\n",
        "# Unzip the files\n",
        "!unzip /content/inference.zip -d /content/inference"
      ],
      "metadata": {
        "id": "rWVR27wJAlLt",
        "outputId": "9e7e4e04-5c7d-450b-8030-121761b5704b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/14mw0v8Bi-MzhsqSI0K3KO23YrUHttM7P/Zindi-Amazon/inference\n",
            "  adding: Sentinel_AllBands_Inference_2021_02.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2022_02.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2024_01.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2020_03.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2021_04.tif (deflated 4%)\n",
            "  adding: Sentinel_AllBands_Inference_2020_02.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2021_01.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2022_01.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2023_01.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2020_01.tif (deflated 5%)\n",
            "  adding: Sentinel_AllBands_Inference_2021_03.tif (deflated 5%)\n",
            "Archive:  /content/inference.zip\n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2021_02.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2022_02.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2024_01.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2020_03.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2021_04.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2020_02.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2021_01.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2022_01.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2023_01.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2020_01.tif  \n",
            "  inflating: /content/inference/Sentinel_AllBands_Inference_2021_03.tif  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download inference images from Team TerraPulse's Google Drive\n",
        "\n",
        "**Note**: If you want to work with inference images generated by team TerraPulse, uncomment the 2 cells below."
      ],
      "metadata": {
        "id": "RP7EO8oUIWtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ID of a zip archive in Team TerraPulse's Google Drive\n",
        "# file_id = \"1HbfE0BFF9CVukUlhl6hUKlujPtAth7BU\"\n",
        "# url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# # Download zip archive\n",
        "# gdown.download(url, \"inference.zip\", quiet=False)"
      ],
      "metadata": {
        "id": "ybvMWED1SsBy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip archive\n",
        "# !unzip inference.zip -d /content"
      ],
      "metadata": {
        "id": "OInRXFNlT4ef"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4hf8Z5CwSOE"
      },
      "source": [
        "## Get list of AOIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7UQeErEItTQ"
      },
      "source": [
        "AOI's are stored in team TerraPulse's GitHub repo (`zindi-amazon-secret-runway`). The following cell creates a new folder `zindi-amazon-secret-runway` in Google Colab runtime. Shapefiles for the individual AOI's are located in this folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4yu4Ze-YT7m",
        "outputId": "ebf93f7b-1aee-4253-9b20-bf81353e6237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'zindi-amazon-secret-runway' already exists and is not an empty directory.\n",
            "/content/drive/.shortcut-targets-by-id/14mw0v8Bi-MzhsqSI0K3KO23YrUHttM7P/Zindi-Amazon/inference/zindi-amazon-secret-runway\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# Define the repository path\n",
        "repo_path = '/content/zindi-amazon-secret-runway'\n",
        "\n",
        "# Check if the directory already exists\n",
        "if os.path.exists(repo_path):\n",
        "    # Remove the existing directory to start fresh\n",
        "    shutil.rmtree(repo_path)\n",
        "    print(f\"Removed existing directory: {repo_path}\")\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/liangchow/zindi-amazon-secret-runway.git\n",
        "\n",
        "# Move into the repository directory\n",
        "%cd zindi-amazon-secret-runway\n",
        "\n",
        "# Pull the latest changes\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_aoi_path = '/content/zindi-amazon-secret-runway/Data_Visualization/data/shp_test_AOIs'"
      ],
      "metadata": {
        "id": "RUPWBk0qC4k9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yUJFRoSrYsLr"
      },
      "outputs": [],
      "source": [
        "aoi_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all shapefiles in the folder\n",
        "shapefiles = glob.glob(f'{base_aoi_path}/*.shp')\n",
        "\n",
        "# Extract `year_month` from each file path\n",
        "for shp in shapefiles:\n",
        "    match = re.search(r'(\\d{4}_\\d{2})', shp)  # Looks for pattern like 2021_04\n",
        "    if match:\n",
        "        aoi_list.append(match.group(1))\n",
        "\n",
        "print(f\"Total number of AOIs: {len(aoi_list)}\")\n",
        "print('\\n'.join([f\"{index}: {item}\" for index, item in enumerate(aoi_list)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hy3-MPFcE9p",
        "outputId": "a8707e01-f08d-4746-a1d1-d63fd7489037"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of AOIs: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model from your Google Drive\n",
        "\n",
        "**Note**: If you are working with your own data, edit the path to your model in the cell below"
      ],
      "metadata": {
        "id": "JToj-lT0WpuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"zindi_best_model_20241030_20m_lr0005_B4_B3_B2_B8_VV\"\n",
        "\n",
        "# Specify the file path of your model\n",
        "model_path = f\"/content/drive/MyDrive/Zindi-Amazon/models/{model_name}.pth\"\n",
        "\n",
        "# Move the file to the root directory of Colab\n",
        "shutil.copy(model_path, f\"/content/{model_name}.pth\")"
      ],
      "metadata": {
        "id": "HN9o63TUJczq",
        "outputId": "de390572-2b2f-4db4-a102-f23bffebc81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/zindi_best_model_20241030_20m_lr0005_B4_B3_B2_B8_VV.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model from Team TerraPulse's Google Drive\n",
        "\n",
        "**Note**: If you want to work with model file generated by team TerraPulse, uncomment the cell below"
      ],
      "metadata": {
        "id": "sIRLmCisIKwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model file name - Zindi-Amazon/models/zindi_best_model_20241030_20m_lr0005_B4_B3_B2_B8_VV.pth\n",
        "# # Link to model file - https://drive.google.com/file/d/1-IbCXlVxmwLTLg7utU2E7A2uYB0D80A_/view?usp=drive_link\n",
        "\n",
        "# # Extracted file id from link to model file\n",
        "# file_id = \"1-IbCXlVxmwLTLg7utU2E7A2uYB0D80A_\"\n",
        "# url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# gdown.download(url, \"zindi_best_model_20241030_20m_lr0005_B4_B3_B2_B8_VV.pth\", quiet=False)"
      ],
      "metadata": {
        "id": "MgG3_DybWujR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to run inference on satellite image for an aoi for selected model"
      ],
      "metadata": {
        "id": "Usl46jf3gRrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and load model\n",
        "\n",
        "We used B4, B3 and B2 bands from Sentinel 2 satellite images as default for all models we ran. For some models, NIR (Sentinel 2) and VV (Sentinel 1) bands were included in the model."
      ],
      "metadata": {
        "id": "ZjEaOyPz4lrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNet model with ResNet50 encoder\n",
        "def get_model(channels):\n",
        "    model = smp.Unet(\n",
        "        encoder_name='resnet50', # Choose encoder\n",
        "        encoder_weights='imagenet', # Use pre-trained weights\n",
        "        in_channels=channels,\n",
        "        classes=1, # Binary segmentation\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def load_model(model_file, additional_bands):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained model for inference, configuring it for the specified number of input channels based on default and additional bands.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): The name of the model file to load. The function expects the model file to be stored at the specified `shared_path`.\n",
        "    - additional_bands (list of str): A list of additional band names (e.g., \"B8\", \"VV\") to include as input channels, in addition to the default RGB bands (B4, B3, B2).\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The loaded PyTorch model, set to evaluation mode and moved to the appropriate device (GPU or CPU).\n",
        "\n",
        "    Process:\n",
        "    1. Sets the computation device to GPU if available, otherwise defaults to CPU.\n",
        "    2. Calculates the total number of input channels required by the model, based on the default RGB bands and any additional bands provided.\n",
        "    3. Loads the specified pre-trained model and transfers it to the selected device.\n",
        "    4. Configures the model to evaluation mode and checks the model's input channel configuration, printing details to confirm compatibility.\n",
        "    5. Returns the loaded model for inference.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    model = load_model(\"resnet50\", additional_bands=[\"B8\", \"VV\"])\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Calculate total number of channels\n",
        "    defualt_bands = ['B4', 'B3', 'B2']\n",
        "    channels = len(defualt_bands) + len(additional_bands)\n",
        "\n",
        "    model = get_model(channels).to(device)\n",
        "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    print('Model file {} successfully loaded.'.format(model_name))\n",
        "\n",
        "    # Check the number of input channels in the model\n",
        "    input_channels = model.encoder.conv1.in_channels\n",
        "    print(f\"The model expects {input_channels} input channels.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KxbAHfdmqU6N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load AOI boundary"
      ],
      "metadata": {
        "id": "qv28EWKZk7RT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Oh_8NeAcs3qS"
      },
      "outputs": [],
      "source": [
        "def get_aoi_boundary(aoi_name):\n",
        "    \"\"\"\n",
        "    Loads the boundary of a specified Area of Interest (AOI) from a shapefile and retrieves its coordinate reference system (CRS).\n",
        "\n",
        "    Parameters:\n",
        "    - aoi_name (str): The identifier for the AOI, used to locate the corresponding shapefile.\n",
        "\n",
        "    Returns:\n",
        "    - aoiboundary (GeoDataFrame): A GeoDataFrame containing the boundary geometry of the specified AOI.\n",
        "    - aoicrs (str): The coordinate reference system (CRS) of the AOI boundary.\n",
        "\n",
        "    Process:\n",
        "    1. Constructs the file path to the shapefile using the provided AOI name.\n",
        "    2. Reads the shapefile into a GeoDataFrame, which includes the AOI boundary geometry.\n",
        "    3. Extracts the CRS of the AOI boundary for reference in further geospatial operations.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    aoiboundary, aoicrs = get_aoi_boundary('2021_01')\n",
        "    ```\n",
        "\n",
        "    Raises:\n",
        "    - FileNotFoundError: If the specified AOI shapefile does not exist at the constructed path.\n",
        "    \"\"\"\n",
        "\n",
        "    shapefile = os.path.join(base_aoi_path, f'aoi_{aoi_name}.shp')\n",
        "    aoiboundary = gpd.read_file(shapefile )\n",
        "    aoicrs = aoiboundary.crs\n",
        "    return aoiboundary, aoicrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfTOeimJeqJ"
      },
      "source": [
        "## Download inference image for AOI and select the bands\n",
        "\n",
        "The satellite images used for inference have 9 bands from both Sentinel 1 and Sentinel 2. This function fetches the satellite image for an AOI using the Rasterio library from a shared folder in Google Drive. It creates a new image with a subset of the bands depending on the model and saves it in the `working` folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_image(aoi, transform=None, additional_bands=[]):\n",
        "    \"\"\"\n",
        "    Extracts specific bands from a Sentinel satellite image, rescales RGB bands, and saves a subset of the bands\n",
        "    to a new GeoTIFF file.\n",
        "\n",
        "    Parameters:\n",
        "    - aoi (str): The area of interest identifier, used to locate the specific image file.\n",
        "    - transform (optional): Placeholder parameter for any future transformation functionality.\n",
        "    - additional_bands (list of str, optional): List of additional band names (e.g., \"B8\", \"VV\") to include in the output image\n",
        "      along with the RGB bands (B4, B3, B2). Default is an empty list.\n",
        "\n",
        "    Returns:\n",
        "    - output_file (str): The path to the newly created TIFF file containing the selected bands.\n",
        "\n",
        "    Process:\n",
        "    1. Constructs the image file path for the specified AOI.\n",
        "    2. Opens the image and retrieves the specified bands (B4, B3, B2, plus any additional bands).\n",
        "    3. Validates that all required bands are present in the source image.\n",
        "    4. Rescales the RGB bands (B4, B3, B2) to a range of [0, 1] by clipping values to 2000 and normalizing.\n",
        "    5. Stacks additional bands (if provided) with the RGB bands to create a multi-band array.\n",
        "    6. Writes the resulting array to a new TIFF file with updated metadata for the specified bands.\n",
        "    7. Saves the file in the specified directory and returns the file path.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    image_path = os.path.join('/content/inference/', 'Sentinel_AllBands_Inference_{}.tif'.format(aoi))\n",
        "\n",
        "    with rasterio.open(image_path) as src:\n",
        "\n",
        "        # Initialize dictionary for all required bands\n",
        "        bands = {desc: src.read(i + 1) for i, desc in enumerate(src.descriptions) if desc in ['B4', 'B3', 'B2'] + additional_bands}\n",
        "\n",
        "        # Ensure all specified bands are present\n",
        "        required_bands = ['B4', 'B3', 'B2'] + additional_bands\n",
        "        for band in required_bands:\n",
        "            if band not in bands:\n",
        "                raise ValueError(f\"Missing band {band} in image: {image_path}\")\n",
        "\n",
        "        # Update metadata for the new file\n",
        "        out_meta = src.meta.copy()\n",
        "        out_meta.update({\n",
        "            \"count\": len(required_bands)  # Number of bands to save in the new file\n",
        "        })\n",
        "\n",
        "        # Rescale RGB bands (B4, B3, B2) only\n",
        "        rgb_image = np.stack([bands['B4'], bands['B3'], bands['B2']], axis=-1)\n",
        "        rgb_image = np.clip(rgb_image, 0, 2000) / 2000  # Scaling between 0 and 1\n",
        "\n",
        "        # Stack additional bands without normalization\n",
        "        other_band_images = [bands[band] for band in additional_bands]\n",
        "        if other_band_images:\n",
        "            full_image = np.concatenate([rgb_image] + [np.expand_dims(b, axis=-1) for b in other_band_images], axis=-1)\n",
        "        else:\n",
        "            full_image = rgb_image\n",
        "\n",
        "        band_data = np.moveaxis(full_image, -1, 0)  # Reorder to (bands, height, width)\n",
        "\n",
        "        # Output file\n",
        "        output_file = os.path.join(working_path, 'Sentinel_AllBands_Inference_{}_subset.tif'.format(aoi_name))\n",
        "\n",
        "        # Write the subset of bands to a new TIFF file\n",
        "        with rasterio.open(output_file, \"w\", **out_meta) as dest:\n",
        "            dest.write(band_data)\n",
        "            print(f\"New TIFF file with selected bands created at: {output_file}\")\n",
        "\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "h7j-JdBBElCs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate tiles for inference image\n",
        "\n",
        "During training, the model learned to recognize patterns within a specific patch size. The inference image is split up into same sized patches before running inference. This function creates geosjon file containing tiles for an AOI and saves it in the `working` folder."
      ],
      "metadata": {
        "id": "xzaHSHAPSZwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define patch size\n",
        "patch_size = 224"
      ],
      "metadata": {
        "id": "ByBwuJzmSoWp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fqbY3WyKaAqf"
      },
      "outputs": [],
      "source": [
        "def generate_tiles(image_file, output_file, aoi_name, size=224):\n",
        "    \"\"\"\n",
        "    Generates a grid of square polygon tiles of a specified size from a georeferenced raster image and saves them as a GeoJSON file.\n",
        "\n",
        "    Args:\n",
        "      image_file (str): Path to the input raster image file (e.g., .tif format).\n",
        "      output_file (str): Path to save the output GeoJSON file containing the generated tiles.\n",
        "      aoi_name (str): Name of the Area of Interest (AOI), used in naming the output file.\n",
        "      size (int, optional): Size of each square tile in pixels (default is 224).\n",
        "\n",
        "    Returns:\n",
        "      GeoDataFrame: A GeoDataFrame containing a grid square polygon tiles.\n",
        "\n",
        "    Process:\n",
        "    1. Opens the input raster image and retrieves its dimensions.\n",
        "    2. Creates a sliding window over the image dimensions to define square tiles of the specified size.\n",
        "    3. For each tile, calculates its georeferenced bounding box and generates a corresponding polygon.\n",
        "    4. Assigns a unique ID to each tile polygon and stores it in a dictionary.\n",
        "    5. Converts the dictionary to a GeoDataFrame, sets its coordinate reference system (CRS), and saves it as a GeoJSON file.\n",
        "    6. Closes the raster file and returns the resulting GeoDataFrame.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    tiles_gdf = generate_tiles(\"input_image.tif\", \"output_tiles.geojson\", \"2020_01\", size=224)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the raster image using rasterio\n",
        "    raster = rio.open(image_file)\n",
        "    width, height = raster.shape\n",
        "\n",
        "    # Create a dictionary which will contain our 224 x 224 px polygon tiles\n",
        "    # Later we'll convert this dict into a GeoPandas DataFrame.\n",
        "    geo_dict = { 'id' : [], 'geometry' : []}\n",
        "    index = 0\n",
        "\n",
        "    # Do a sliding window across the raster image\n",
        "    with tqdm(total=width*height) as pbar:\n",
        "      for w in range(0, width, size):\n",
        "          for h in range(0, height, size):\n",
        "              # Create a Window of your desired size\n",
        "              window = rio.windows.Window(h, w, size, size)\n",
        "              # Get the georeferenced window bounds\n",
        "              bbox = rio.windows.bounds(window, raster.transform)\n",
        "              # Create a shapely geometry from the bounding box\n",
        "              bbox = box(*bbox)\n",
        "\n",
        "              # Create a unique id for each geometry\n",
        "              uid = '{}-{}'.format(aoi_name.lower().replace(' ', '_'), index)\n",
        "\n",
        "              # Update dictionary\n",
        "              geo_dict['id'].append(uid)\n",
        "              geo_dict['geometry'].append(bbox)\n",
        "\n",
        "              index += 1\n",
        "              pbar.update(size*size)\n",
        "\n",
        "    # Cast dictionary as a GeoPandas DataFrame\n",
        "    results = gpd.GeoDataFrame(pd.DataFrame(geo_dict))\n",
        "    # Set CRS to AOI crs\n",
        "    results.crs = {'init' : aoicrs}\n",
        "    # Save file as GeoJSON\n",
        "    results.to_file(output_file, driver=\"GeoJSON\")\n",
        "    print(f\"Tiles saved to: {output_file}\")\n",
        "\n",
        "    raster.close()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P2gEnIHqBY0"
      },
      "source": [
        "## Define data transformations\n",
        "\n",
        "A set of data transformations to the test set were applied during training. Before running the inference images through the model, these same transformations are applied to the inference image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentations using albumentations and PyTorch's ToTensor\n",
        "def get_augmentations(option='inference', optional_bands=None):\n",
        "    \"\"\"\n",
        "    Creates an augmentation pipeline for satellite images using Albumentations, with optional normalization for additional bands.\n",
        "\n",
        "    Parameters:\n",
        "    - option (str): The phase of the augmentation, which determines the transformations applied.\n",
        "                    Must be one of 'train', 'val', 'test', or 'inference'.\n",
        "    - optional_bands (list of str, optional): List of additional band names (e.g., \"B8\", \"B11\") to include in normalization\n",
        "                                              beyond the default RGB bands (B4, B3, B2). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    - A.Compose: An Albumentations composition of transformations to be applied to the images.\n",
        "\n",
        "    Process:\n",
        "    1. Defines the mean and standard deviation for RGB bands and additional optional bands for normalization.\n",
        "    2. Combines the base and optional band statistics as needed to compute the mean and standard deviation.\n",
        "    3. Based on the specified `option` (e.g., 'train', 'val'), sets up an augmentation pipeline with transformations such as cropping, flipping, and rotation.\n",
        "    4. Adds normalization using the computed mean and standard deviation and converts images to tensors.\n",
        "    5. Returns the composed augmentation pipeline ready to be applied to satellite imagery.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    augmentations = get_augmentations(option='inference', optional_bands=['B8', 'VV'])\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    # Define mean and std for RGB (B4, B3, B2)\n",
        "    base_band_means = {'B4': 0.485, 'B3': 0.456, 'B2': 0.406}\n",
        "    base_band_stds = {'B4': 0.229, 'B3': 0.224, 'B2': 0.225}\n",
        "\n",
        "    # Define mean and std for additional optional bands\n",
        "    optional_band_means = {'B8': 2987.760, 'B11': 1656.303, 'B12': 729.068, 'VV': -8.048, 'VH': -14.456, 'VV_VH_Ratio': 0.547}\n",
        "    optional_band_stds = {'B8': 504.165, 'B11': 314.123, 'B12': 203.183, 'VV': 1.911, 'VH': 1.825, 'VV_VH_Ratio': 0.064}\n",
        "\n",
        "    # Combine base bands and optional bands\n",
        "    all_band_means = {**base_band_means, **optional_band_means}\n",
        "    all_band_stds = {**base_band_stds, **optional_band_stds}\n",
        "\n",
        "    # If optional_bands is specified, get only those; otherwise, default to RGB\n",
        "    selected_bands = ['B4', 'B3', 'B2'] + (optional_bands if optional_bands else [])\n",
        "    mean = tuple(all_band_means[band] for band in selected_bands)\n",
        "    std = tuple(all_band_stds[band] for band in selected_bands)\n",
        "\n",
        "    # # Define augmentation pipeline based on phase\n",
        "    augmentations = {\n",
        "        'train': [\n",
        "            A.CropNonEmptyMaskIfExists(width=224, height=224),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "        ],\n",
        "        'val': [\n",
        "            A.CropNonEmptyMaskIfExists(width=224, height=224),\n",
        "        ],\n",
        "        'test': [\n",
        "            A.CropNonEmptyMaskIfExists(width=224, height=224),\n",
        "        ],\n",
        "        'inference': [\n",
        "            A.CenterCrop(width=224, height=224),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    if option not in augmentations:\n",
        "        raise ValueError(\"Invalid option. Choose from 'train', 'val', 'test', or 'inference'.\")\n",
        "\n",
        "    # Add normalization and tensor transformation\n",
        "    augmentations[option].extend([\n",
        "        A.Normalize(mean=mean, std=std, max_pixel_value=1.0),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    # Return the Compose pipeline\n",
        "    return A.Compose(augmentations[option])"
      ],
      "metadata": {
        "id": "nGMNBni1uYEQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a subfolder to store model outputs"
      ],
      "metadata": {
        "id": "3BPQRqbeDFjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_folder(path, name):\n",
        "    \"\"\"\n",
        "    Creates a new folder in the specified directory if it does not already exist, and returns the path.\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): The base directory where the new folder should be created.\n",
        "    - name (str): The name of the new folder to be created within the specified path.\n",
        "\n",
        "    Returns:\n",
        "    - local_folder_path (str): The full path to the newly created or existing folder.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    folder_path = create_model_folder(\"/content/models\", \"model_xyz\")\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the path for the new folder\n",
        "    local_folder_path = path + '/' + name\n",
        "\n",
        "    # Create the folder in colab runtine if it doesn't already exist\n",
        "    if not os.path.exists(local_folder_path):\n",
        "        os.makedirs(local_folder_path)\n",
        "        print(f\"Folder created: {local_folder_path}\")\n",
        "    else:\n",
        "        print(f\"Folder already exists: {local_folder_path}\")\n",
        "\n",
        "    return local_folder_path"
      ],
      "metadata": {
        "id": "sEtspV77DJ3p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pad image tile"
      ],
      "metadata": {
        "id": "2-SNO0LEOepW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_image_patch(image, target_size=224):\n",
        "    \"\"\"\n",
        "    Pads an image to a specified target size with zeroes if the image dimensions are smaller than the target.\n",
        "\n",
        "    Args:\n",
        "      image (np.ndarray): The input image array in (C, H, W) format, where C is the number of channels,\n",
        "                          H is the height, and W is the width.\n",
        "      target_size (int, optional): The target size for both height and width. Defaults to 224.\n",
        "\n",
        "    Returns:\n",
        "      tuple:\n",
        "        - padded_image (np.ndarray): The padded image array in (C, H, W) format, resized to `target_size` x `target_size`.\n",
        "        - padding (tuple): The padding applied as (top, bottom, left, right) for height and width.\n",
        "\n",
        "    Process:\n",
        "    1. Calculates the necessary padding for the height and width dimensions if they are smaller than `target_size`.\n",
        "    2. Pads the image symmetrically with zeroes on each side (top, bottom, left, right).\n",
        "    3. If no padding is required, returns the original image.\n",
        "    4. Returns the padded image along with the padding details.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    padded_image, padding = pad_image_patch(image, target_size=224)\n",
        "    ```\n",
        "\n",
        "    Notes:\n",
        "    - The padding is only applied if the height or width of the image is less than `target_size`.\n",
        "    \"\"\"\n",
        "\n",
        "    channels, height, width = image.shape\n",
        "\n",
        "    # Calculate padding for height and width\n",
        "    pad_top = (target_size - height) // 2 if height < target_size else 0\n",
        "    pad_bottom = target_size - height - pad_top if height < target_size else 0\n",
        "    pad_left = (target_size - width) // 2 if width < target_size else 0\n",
        "    pad_right = target_size - width - pad_left if width < target_size else 0\n",
        "\n",
        "    # Apply padding only if necessary\n",
        "    if pad_top > 0 or pad_bottom > 0 or pad_left > 0 or pad_right > 0:\n",
        "        padded_image = np.pad(image,\n",
        "                              ((0, 0), (pad_top, pad_bottom), (pad_left, pad_right)),\n",
        "                              mode='constant',\n",
        "                              constant_values=0)\n",
        "    else:\n",
        "        padded_image = image  # No padding needed\n",
        "\n",
        "    # Return the padded image and the padding details\n",
        "    return padded_image, (pad_top, pad_bottom, pad_left, pad_right)"
      ],
      "metadata": {
        "id": "WY9VC2_RYXCG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference/prediction for each image tile"
      ],
      "metadata": {
        "id": "13xOEgN4WyTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-u05VmCkdGv8"
      },
      "outputs": [],
      "source": [
        "def predict_airstrip(image, shape, model, optional_bands=None):\n",
        "    \"\"\"\n",
        "    Generates model predictions for airstrip detection on a specified image tile using a trained model.\n",
        "\n",
        "    Args:\n",
        "      image (str): Path to the input image file (in .tiff format).\n",
        "      shape (geometry): Polygon geometry defining the tile area to crop from the input image.\n",
        "      model (torch.nn.Module): Trained PyTorch model used for prediction.\n",
        "      optional_bands (list of str, optional): List of additional band names to include in the input, beyond the default bands.\n",
        "\n",
        "    Returns:\n",
        "      tuple:\n",
        "        - predictions (torch.Tensor): Model's binary prediction tensor for the airstrip, where values > 0.5 are considered positive.\n",
        "        - padding (tuple): The padding applied as (top, bottom, left, right) to make the cropped image match the model's input size.\n",
        "\n",
        "    Process:\n",
        "    1. Opens the input image and crops it to the specified tile shape.\n",
        "    2. Pads the cropped image patch to match the model’s required input size.\n",
        "    3. Validates that the padded image has the correct number of bands to match the model’s expected input channels.\n",
        "    4. Saves the padded, cropped image as a temporary TIFF file for further processing.\n",
        "    5. Loads the temporary file, applies preprocessing and transformations using the defined augmentation pipeline.\n",
        "    6. Runs the processed image through the model to generate predictions.\n",
        "    7. Applies a sigmoid function to the model's output to get probabilities, and thresholds the output to generate binary predictions.\n",
        "    8. Returns the prediction tensor along with the padding information.\n",
        "\n",
        "    Raises:\n",
        "    - Exception: If the number of bands in the input image does not match the model’s expected number of input channels.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    predictions, padding = predict_airstrip(\"path/to/image.tiff\", tile_geometry, model, optional_bands=[\"B8\", \"B11\"])\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    with rio.open(image) as src:\n",
        "        # Crop source image using polygon shape\n",
        "        # See more information here:\n",
        "        # https://rasterio.readthedocs.io/en/latest/api/rasterio.mask.html#rasterio.mask.mask\n",
        "        out_image, out_transform = rio.mask.mask(src, shape, crop=True)\n",
        "\n",
        "        # Pad image patch size\n",
        "        padded_image, padding = pad_image_patch(out_image, patch_size)\n",
        "\n",
        "        # Throw error if image bands dont match\n",
        "        if padded_image.shape[0] != model.encoder.conv1.in_channels:\n",
        "          raise Exception(\"Number of bands don't match\")\n",
        "\n",
        "        # Get the metadata of the source image and update it\n",
        "        # with the width, height, and transform of the cropped image\n",
        "        out_meta = src.meta\n",
        "        out_meta.update({\n",
        "              \"driver\": \"GTiff\",\n",
        "              \"height\": padded_image.shape[1],\n",
        "              \"width\": padded_image.shape[2],\n",
        "              \"transform\": out_transform\n",
        "        })\n",
        "\n",
        "        # Save the cropped image as a temporary TIFF file.\n",
        "        temp_tif = os.path.join(working_path, 'temp.tif')\n",
        "        with rio.open(temp_tif, \"w\", **out_meta) as dest:\n",
        "          dest.write(padded_image)\n",
        "\n",
        "        # Open the cropped image and generated prediction\n",
        "        # using the trained Pytorch model\n",
        "\n",
        "        # Use rasterio to read the image instead of Pillow\n",
        "        # to avoid UnidentifiedImageError(msg)\n",
        "        with rio.open(temp_tif) as dataset:\n",
        "            image_data = dataset.read()\n",
        "            image_data = image_data.transpose(1, 2, 0)  # Reorder dimensions to (height, width, channels)\n",
        "\n",
        "        # Get the augmentation pipeline for inference\n",
        "        augmentation_pipeline = get_augmentations(option='inference', optional_bands=optional_bands)\n",
        "\n",
        "        # Apply the transformation to the image data\n",
        "        augmented = augmentation_pipeline(image=image_data)\n",
        "\n",
        "        input = augmented['image']  # Extract the transformed image tensor\n",
        "\n",
        "        # Get the model's output\n",
        "        output = model(input.unsqueeze(0))\n",
        "\n",
        "        # Apply sigmoid if necessary to get probabilities between 0 and 1\n",
        "        output = torch.sigmoid(output) # Uncomment if model outputs logits instead of probabilities\n",
        "\n",
        "        # Threshold to get binary predictions (0 or 1)\n",
        "        predictions = (output > 0.5).int() # Adjust threshold if needed\n",
        "\n",
        "        # Return the entire prediction tensor\n",
        "        return (predictions, padding)\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a predictions raster"
      ],
      "metadata": {
        "id": "sv4pnXNZRztB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_mosaic_predictions(tiles, inference_image_path, model_name, aoi_name, working_path):\n",
        "    \"\"\"\n",
        "    Processes predictions for each tile in a GeoDataFrame, trims padding if applied, and mosaics all predictions into\n",
        "    a single output TIFF file matching the extent and size of the original inference image.\n",
        "\n",
        "    Args:\n",
        "      tiles (GeoDataFrame): GeoDataFrame containing tile geometries, padding information, and model predictions.\n",
        "      inference_image_path (str): Path to the original inference image file.\n",
        "      model_name (str): Name of the model used for predictions, used in naming the output file.\n",
        "      aoi_name (str): Name of the Area of Interest (AOI), used in naming the output file.\n",
        "      working_path (str): Directory path to save the final mosaic TIFF file.\n",
        "\n",
        "    Returns:\n",
        "      str: Path to the saved mosaic TIFF file.\n",
        "\n",
        "    Process:\n",
        "    1. Opens the original inference image to retrieve profile metadata and determine the mosaic’s shape and extent.\n",
        "    2. Initializes an empty mosaic array to accumulate predictions across all tiles.\n",
        "    3. Iterates over each tile in the GeoDataFrame:\n",
        "       - Extracts predictions and padding information.\n",
        "       - Trims padding from the prediction array if padding was applied.\n",
        "       - Computes the correct location in the mosaic for the tile based on its geometry bounds.\n",
        "       - Resizes the prediction if necessary to fit into the mosaic slice.\n",
        "       - Places the prediction in the appropriate mosaic location.\n",
        "    4. Updates the mosaic's metadata to match the inference image and saves the result as a single-band GeoTIFF file.\n",
        "    5. Visualizes the final mosaic and prints the output path.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    output_mosaic_path = process_and_mosaic_predictions(tiles_gdf, \"path/to/inference_image.tif\",\n",
        "                                                        model_name=\"model_xyz\", aoi_name=\"2020-01\",\n",
        "                                                        working_path=\"/content/\")\n",
        "    ```\n",
        "\n",
        "    Notes:\n",
        "    - Padding is removed from each tile's prediction as needed before adding it to the mosaic.\n",
        "    - The final mosaic file is saved in the specified working directory, organized by model and AOI name.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the inference image to get its profile and dimensions\n",
        "    with rio.open(inference_image_path) as src:\n",
        "        profile = src.profile.copy()\n",
        "        mosaic_shape = (profile['height'], profile['width'])  # Full extent size\n",
        "        mosaic = np.zeros(mosaic_shape, dtype=np.uint8)  # Initialize blank mosaic\n",
        "\n",
        "    # Process predictions for each tile\n",
        "    print(f\"Predicting pixel values for tiles: {aoi_name}\")\n",
        "    for tile_index in tqdm(range(len(tiles)), desc=\"Processing tiles\"):\n",
        "        # Retrieve the prediction from the GeoDataFrame\n",
        "        predictions = tiles.iloc[tile_index]['predictions']\n",
        "        # Convert predictions from torch tensor to numpy array for easier processing\n",
        "        predictions_np = predictions.squeeze().cpu().numpy()\n",
        "\n",
        "        # Retrieve padding from the GeoDataFrame\n",
        "        padding = tiles.iloc[tile_index]['padding']  # Assume padding is in (top, bottom, left, right\n",
        "        # Trim padding from prediction numpy array if applied\n",
        "        padding = tiles.iloc[tile_index]['padding']  # Assume padding is in (top, bottom, left, right) format\n",
        "        if padding:\n",
        "            pad_top, pad_bottom, pad_left, pad_right = padding\n",
        "            predictions_np = predictions_np[\n",
        "                pad_top: predictions_np.shape[0] - pad_bottom,\n",
        "                pad_left: predictions_np.shape[1] - pad_right\n",
        "            ]\n",
        "\n",
        "        # Calculate the tile's position in the mosaic array based on its geometry bounds\n",
        "        tile_geometry = tiles.iloc[tile_index]['geometry']\n",
        "        minx, miny, maxx, maxy = tile_geometry.bounds\n",
        "        tile_transform = from_bounds(minx, miny, maxx, maxy, predictions_np.shape[1], predictions_np.shape[0])\n",
        "\n",
        "        # Convert the tile transform to pixel coordinates in the full mosaic\n",
        "        row_start, col_start = rio.transform.rowcol(profile['transform'], minx, maxy)\n",
        "        row_end, col_end = rio.transform.rowcol(profile['transform'], maxx, miny)\n",
        "\n",
        "        # Ensure coordinates are integers and within bounds\n",
        "        row_start, col_start = int(max(0, row_start)), int(max(0, col_start))\n",
        "        row_end, col_end = int(min(mosaic_shape[0], row_end)), int(min(mosaic_shape[1], col_end))\n",
        "\n",
        "        # Check if prediction is same as mosaic slice\n",
        "        if (row_end - row_start, col_end - col_start) != predictions_np.shape:\n",
        "          print(f\"Prediction size: {predictions_np.shape}\")\n",
        "          print(f\"Mosaic slice size: {(row_end - row_start, col_end - col_start)}\")\n",
        "\n",
        "        # Place the tile's prediction in the correct location within the mosaic array\n",
        "        mosaic[row_start:row_end, col_start:col_end] = predictions_np\n",
        "\n",
        "    # Update metadata for the mosaic to match the inference image\n",
        "    profile.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"count\": 1  # Single band for predictions\n",
        "    })\n",
        "\n",
        "    # Save the mosaic as a GeoTIFF file on the shared drive\n",
        "    output_predictions_mosaic = os.path.join(working_path, model_name, f\"aoi_{aoi_name}_prediction_mosaic.tif\")\n",
        "    os.makedirs(os.path.dirname(output_predictions_mosaic), exist_ok=True)\n",
        "    with rio.open(output_predictions_mosaic, \"w\", **profile) as dest:\n",
        "        dest.write(mosaic, 1)\n",
        "\n",
        "    # Visualize the mosaic\n",
        "    show(mosaic, transform=profile['transform'])\n",
        "    print(f\"Prediction mosaic saved to: {output_predictions_mosaic}\")\n",
        "\n",
        "    return output_predictions_mosaic\n",
        "\n"
      ],
      "metadata": {
        "id": "9FHKai4N-l7d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up working directory\n",
        "!rm -rf /content/working/*"
      ],
      "metadata": {
        "id": "wurD5cBpG-0D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model inference"
      ],
      "metadata": {
        "id": "LZ5BMuWS151B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'model_20241030_20m_lr0005_B4_B3_B2_B8_VV'\n",
        "print(f\"Selected Model: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1jXek2iAKI3",
        "outputId": "62d8d132-a789-4ec7-c72e-519c7a5629c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Model: model_20241030_20m_lr0005_B4_B3_B2_B8_VV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_bands = ['B8', 'VV']"
      ],
      "metadata": {
        "id": "nFSCbmjCAOpp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(f'/content/zindi_best_{model_name}.pth', additional_bands)\n",
        "local_folder_path = create_model_folder(working_path, model_name)\n",
        "shared_folder_path = create_model_folder(working_path, model_name)"
      ],
      "metadata": {
        "id": "K6yJVwn54Ces",
        "outputId": "ed9ffe22-4199-4976-f47e-c6fec1eeda49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 158MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model file model_20241030_20m_lr0005_B4_B3_B2_B8_VV successfully loaded.\n",
            "The model expects 5 input channels.\n",
            "Folder created: /content/working/model_20241030_20m_lr0005_B4_B3_B2_B8_VV\n",
            "Folder already exists: /content/working/model_20241030_20m_lr0005_B4_B3_B2_B8_VV\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d073037d95cd>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_file, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for aoi_name in aoi_list:\n",
        "  print(f\"Processing AOI: {aoi_name}\")\n",
        "  aoiboundary, aoicrs = get_aoi_boundary(aoi_name)\n",
        "  print(f\"AOI crs: {aoicrs}\")\n",
        "\n",
        "  # Download inference image\n",
        "  inference_image_path = get_inference_image(aoi_name, additional_bands=additional_bands)\n",
        "\n",
        "  # Check image channels\n",
        "  with rasterio.open(inference_image_path) as src:\n",
        "    print(f\"Number of bands in the image: {src.count}\")\n",
        "    if src.count != model.encoder.conv1.in_channels:\n",
        "      raise Exception(\"Number of bands in the image does not match the expected number\")\n",
        "\n",
        "  # Create geojson file of tiles\n",
        "  print(f\"Split image into tiles...\")\n",
        "  output_patch_file = os.path.join(working_path, 'aoi_{}_{}x{}px_patches.geojson'.format(aoi_name, patch_size, patch_size))\n",
        "  tiles = generate_tiles(inference_image_path, output_patch_file, aoi_name, size=patch_size)\n",
        "\n",
        "  # Commence model prediction\n",
        "  print(\"Make predictions for all image tiles...\")\n",
        "  all_predictions = []  # Store predictions for all tiles\n",
        "  all_padding = []  # Store predictions for all tiles\n",
        "  for index in tqdm(range(len(tiles)), total=len(tiles)):\n",
        "      predictions, padding = predict_airstrip(inference_image_path, [tiles.iloc[index]['geometry']], model, additional_bands)\n",
        "      all_predictions.append(predictions)\n",
        "      all_padding.append(padding)\n",
        "\n",
        "  # Add predictions and padding information to the tiles GeoDataFrame\n",
        "  tiles[\"predictions\"] = all_predictions\n",
        "  tiles[\"padding\"] = all_padding\n",
        "\n",
        "  # Save the GeoDataFrame with predictions and padding\n",
        "  filepath = os.path.join(f\"{working_path}/{model_name}\", \"{}_preds_and_padding.geojson\".format(aoi_name))\n",
        "  tiles.to_file(filepath, driver=\"GeoJSON\")\n",
        "  print(f\"Predictions for all images tiles saved to:\", {filepath})\n",
        "\n",
        "  # Generate a predictions raster\n",
        "  print(\"Mosaic all tiles together to create a predction raster...\")\n",
        "  predictions_mosaic = process_and_mosaic_predictions(tiles, inference_image_path, model_name, aoi_name, working_path)\n",
        "\n",
        "  # Uncomment the following lines to copy mosaic to the shared Google Drive folder\n",
        "  # shutil.copy(predictions_mosaic, shared_folder_path)\n",
        "  # print(f\"Prediction mosaic saved to: {shared_folder_path}\")\n",
        "\n",
        "  print(f\"Finished processing AOI: {aoi_name}\")\n",
        "  print(\"***********************************\")\n"
      ],
      "metadata": {
        "id": "G4R5tBCa19mJ"
      },
      "execution_count": 28,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}